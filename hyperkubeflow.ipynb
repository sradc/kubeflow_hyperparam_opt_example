{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Copyright 2022 Sidney Radcliffe\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toy example of Bayesian hyperparameter optimization on multiple cloud VMs in parallel using Kubeflow (Python)\n",
    "\n",
    "Screenshot of an execution of the Kubeflow pipeline we'll implement, where each node on the graph corresponds to a cloud virtual machine, and the edges to data that's passed forward:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./pipeline_img.png\" width=800>\n",
    "</p>\n",
    "\n",
    "To \"train\" a machine learning model is to carry out some optimization process (e.g. gradient descent). But how do you know which kind of model to optimize in the first place, and which parameters to select for a given model (e.g. number of layers/trees/etc.)? Well, hopefully you can narrow things down a bit via domain knowledge (e.g. \"I know that X type of model tends to work well on this type of data\"), but unless you are very certain about your guess (or you don't particularly need to maximise performance), you'll probably want to carry out some kind of hyperparameter search. Two common techniques for this are grid search and random search, the latter often being used because it would take too long to carry out an exhaustive grid search. A third option is to explicitly treat the hyperparameter search itself as a function, e.g. `evaluate_model(hyperparameters) -> score`, and to use a \"meta\" model to optimize this function; we'll go with this option.\n",
    "\n",
    "The library [Scikit-Optimize](https://scikit-optimize.github.io/) has a toy example (reproduced below) of Bayesian optimization that is parallelized using multiple processes on a single machine. However, this single machine approach won't work well for models that require a lot of resources (e.g. CPU, RAM, GPU), so we'll adapt the example and parallelize the search across multiple cloud machines (the method could be used to train more serious models on beefier machines, and it wouldn't take much to adapt it to random search/grid search).\n",
    "\n",
    "(Why parallelize in the first place?: To be able to run more experiments in a given length of time and so hopefully get better results within that time.)\n",
    "\n",
    "### What is Kubeflow?\n",
    "\n",
    "[Kubeflow](https://www.kubeflow.org/) is a framework that can be used for writing and running machine learning pipelines. We'll use its [function-based components](https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/) to do all our work within Python; writing the code the VMs will execute, specifying the data the VMs will pass between eachother, the resources of the VMs, etc. and ultimately compiling all the info into a json and sending it to the cloud. \n",
    "\n",
    "A nice thing about Kubeflow is it provides the versioning of *the execution of code*, and the data/artifacts that were part of the execution (as opposed to the versioning of just the code itself, which is git's domain). This is valuable in the context of machine learning, because a model is a product of not just its code/architecture, but also the data its trained on.\n",
    "\n",
    "If you happen to have a Kubernetes cluster handy, you could run the pipeline on that, but in this post we'll use [GCP's](https://cloud.google.com/) managed service, [Vertex](https://cloud.google.com/vertex-ai) (services from other cloud providers are available). When we run the pipeline on Vertex, GCP will fire up virtual machines for us, and save artifacts to Cloud Storage buckets. (We pay for what we use.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Bayesian hyperparameter optimization from Scikit-Optimize docs\n",
    "\n",
    "Below is the example from the Scikit-Optimize [docs](https://scikit-optimize.github.io/0.9/auto_examples/parallel-optimization.html) that parallelizes the hyperparemeter optimization using multiple CPU cores on a single machine. (Tweaked a bit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use these values in the pipeline as well\n",
    "NUM_ITERATIONS = 4\n",
    "NUM_PARALLEL_TRIALS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8080172110371091\n"
     ]
    }
   ],
   "source": [
    "from skopt import Optimizer, space\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# \"branin\" is a function that takes a list of hyperparameter values and returns a score,\n",
    "# we'll use it in place of a real model.\n",
    "from skopt.benchmarks import branin\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    dimensions=[space.Real(-5.0, 10.0), space.Real(0.0, 15.0)],\n",
    "    random_state=1,\n",
    "    base_estimator=\"gp\",\n",
    ")\n",
    "all_scores_and_params = []\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    # Get a list of points in hyperparameter space to evaluate\n",
    "    hyperparam_vals = optimizer.ask(n_points=NUM_PARALLEL_TRIALS)\n",
    "    # Evaluate the points in parallel\n",
    "    scores = Parallel(n_jobs=NUM_PARALLEL_TRIALS)(\n",
    "        delayed(branin)(v) for v in hyperparam_vals\n",
    "    )\n",
    "    all_scores_and_params.extend(zip(hyperparam_vals, scores))\n",
    "    # Update the optimizer with the results\n",
    "    optimizer.tell(hyperparam_vals, scores)\n",
    "# Print the best score found\n",
    "print(min(optimizer.yi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The example from above written with Kubeflow\n",
    "\n",
    "First import the libraries we'll use. (kfp is the [Kubeflow Pipelines SDK](https://pypi.org/project/kfp/).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import Artifact, Input, Output, pipeline\n",
    "\n",
    "from pseudo_tuple_component import PseudoTuple, pseudo_tuple_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `pseudo_tuple_component` is a Python module I've written to\n",
    "workaround the fact the current version of Kubeflow Pipelines SDK, 1.8,\n",
    "doesn't support aggregating the resuls of multiple components.\n",
    "It involves use of the Python's `inspect`, and `ast`, modules\n",
    "to modify the source code of a function... Code [here](https://github.com/sradc/kubeflow_hyperparam_opt_example/blob/b3ef4d7e01055e27011a4d1311cf9adccf37869e/pseudo_tuple_component.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"scikit-opt-example-pipeline\"\n",
    "with open(\"vertex_config.json\", \"r\") as f:\n",
    "    gcp_cfg = json.load(f)  # I put GCP related stuff in here\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    gcp_cfg[\"credentials_path\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll define \"components\", which are the things that run on a single cloud VM. Note that the imports need to go inside the function, because ultimately the contents of the function will be dumped into a string by kfp and run from within a Docker container. Also the type hints are significant, because `kfp` uses them to work out how to deal with the inputs and outputs to the VMs (and there's limitations to what can be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"scikit-optimize==0.9.0\", \"dill==0.3.6\"],\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "def initialize(\n",
    "    random_state: int,\n",
    "    n_points: int,\n",
    "    optimizer_out: Output[Artifact],\n",
    ") -> NamedTuple(\"Outputs\", [(\"hyperparam_vals\", str)]):\n",
    "    \"\"\"Initialize the optimizer and get the first set of hyperparameter values to evaluate.\"\"\"\n",
    "    import json\n",
    "\n",
    "    import dill\n",
    "    from skopt import Optimizer, space\n",
    "\n",
    "    optimizer = Optimizer(\n",
    "        dimensions=[space.Real(-5.0, 10.0), space.Real(0.0, 15.0)],\n",
    "        random_state=random_state,\n",
    "        base_estimator=\"gp\",\n",
    "    )\n",
    "    hyperparam_vals = optimizer.ask(n_points=n_points)\n",
    "    with open(optimizer_out.path, \"wb\") as f:\n",
    "        dill.dump(optimizer, f)\n",
    "    return (json.dumps(hyperparam_vals),)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"scikit-optimize==0.9.0\", \"dill==0.3.6\"],\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "def evalute_model(\n",
    "    hyperparam_vals: str,\n",
    "    idx: int,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate a model with the given hyperparameter values.\"\"\"\n",
    "    import json\n",
    "\n",
    "    from skopt.benchmarks import branin\n",
    "\n",
    "    params = json.loads(hyperparam_vals)[idx]\n",
    "    score = float(branin(params))\n",
    "    return score\n",
    "\n",
    "# `pseudo_tuple_component` is a custom component I wrote to work around\n",
    "# the fact that KFP doesn't support tuples\n",
    "# of kubeflow artifacts as function args.\n",
    "@pseudo_tuple_component(\n",
    "    packages_to_install=[\"scikit-optimize==0.9.0\", \"dill==0.3.6\"],\n",
    "    base_image=\"python:3.10\",\n",
    "    globals_=globals(),\n",
    "    locals_=locals(),\n",
    ")\n",
    "def update_optimizer(\n",
    "    optimizer_in: Input[Artifact],\n",
    "    hyperparam_vals: str,\n",
    "    scores: PseudoTuple(NUM_PARALLEL_TRIALS, float),\n",
    "    optimizer_out: Output[Artifact],\n",
    ") -> NamedTuple(\"Outputs\", [(\"hyperparam_vals\", str), (\"best_score_found\", float)]):\n",
    "    \"\"\"Update the optimizer with the results of the previous evaluation \n",
    "    and get the next set of hyperparameter values to evaluate.\"\"\"\n",
    "    import json\n",
    "\n",
    "    import dill\n",
    "\n",
    "    with open(optimizer_in.path, \"rb\") as f:\n",
    "        optimizer = dill.load(f)\n",
    "    optimizer.tell(json.loads(hyperparam_vals), scores)\n",
    "    hyperparam_vals = optimizer.ask(n_points=4)\n",
    "    with open(optimizer_out.path, \"wb\") as f:\n",
    "        dill.dump(optimizer, f)\n",
    "    return json.dumps(hyperparam_vals), min(optimizer.yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define the pipeline. Note it isn't actually executed until we compile it and send it to the cloud.\n",
    "We're basically specifying which components to run in what order, and what to pass\n",
    "to each component. We also specify resources for the VMs that will run the \n",
    "components here, (but it's up to the cloud provider whether they respect it, \n",
    "e.g. GCP hasn't given me a machine smaller than 2 CPU, 16GB ram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidneyradcliffe/miniforge3/envs/hyperkubeflow/lib/python3.10/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=gcp_cfg[\"pipeline_root\"],\n",
    ")\n",
    "def my_pipeline(random_state: int = 1):\n",
    "    initialize_ = (\n",
    "        initialize(random_state=random_state, n_points=NUM_PARALLEL_TRIALS)\n",
    "        .set_memory_limit(\"8G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "    )\n",
    "    latest_optimizer = initialize_\n",
    "    for i in range(NUM_ITERATIONS):\n",
    "        scores = {}\n",
    "        for i in range(NUM_PARALLEL_TRIALS):\n",
    "            evalute_model_ = (\n",
    "                evalute_model(\n",
    "                    hyperparam_vals=latest_optimizer.outputs[\"hyperparam_vals\"], idx=i\n",
    "                )\n",
    "                .set_memory_limit(\"8G\")\n",
    "                .set_cpu_limit(\"1\")\n",
    "            )\n",
    "            scores[f\"scores_{i}\"] = evalute_model_.output\n",
    "        latest_optimizer = (\n",
    "            update_optimizer(\n",
    "                optimizer_in=latest_optimizer.outputs[\"optimizer_out\"],\n",
    "                hyperparam_vals=latest_optimizer.outputs[\"hyperparam_vals\"],\n",
    "                **scores,\n",
    "            )\n",
    "            .set_memory_limit(\"8G\")\n",
    "            .set_cpu_limit(\"1\")\n",
    "        )\n",
    "\n",
    "\n",
    "# compile the pipeline into a json that contains\n",
    "# everything needed to run the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline, package_path=f\"{PIPELINE_NAME}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's a matter of sending the pipeline json to the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}_job\",\n",
    "    credentials=credentials,\n",
    "    template_path=f\"{PIPELINE_NAME}.json\",\n",
    "    job_id=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    pipeline_root=gcp_cfg[\"pipeline_root\"],\n",
    "    enable_caching=True,\n",
    "    project=gcp_cfg[\"project_id\"],\n",
    "    location=gcp_cfg[\"region\"],\n",
    ")\n",
    "job.submit(\n",
    "    service_account=gcp_cfg[\"service_account\"], experiment=gcp_cfg[\"experiment_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... after some time... we get the our result, 0.8080302017230245, which is close enough to our local result of 0.8080172110371091.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./pipeline_results.png\" width=600>\n",
    "</p>\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "(In no particular order)\n",
    "\n",
    "- https://scikit-optimize.github.io/0.9/auto_examples/parallel-optimization.html\n",
    "- https://codelabs.developers.google.com/vertex-pipelines-intro\n",
    "- https://www.cloudskillsboost.google/focuses/21234?parent=catalog\n",
    "- https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/\n",
    "- https://www.kubeflow.org/docs/components/pipelines/v2/author-a-pipeline/component-io/\n",
    "- https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#kfp.dsl.ParallelFor\n",
    "- https://github.com/kubeflow/pipelines/issues/1933\n",
    "- https://github.com/kubeflow/pipelines/issues/3412\n",
    "- https://stackoverflow.com/questions/70358400/kubeflow-vs-vertex-ai-pipelines\n",
    "    - \"KubeFlow pipeline stages take a lot less to set up than Vertex in my experience (seconds vs couple of minutes). This was expected, as stages are just containers in KF, and it seems in Vertex full-fledged instances are provisioned to run the containers\"\n",
    "\n",
    "\n",
    "Regarding the necessity of writing `pseudo_tuple_component`.. Kubeflow has [dsl.ParallelFor](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#kfp.dsl.ParallelFor),\n",
    "but there doesn't seem to be a way to aggregate results (see Kubeflow issues [1933](https://github.com/kubeflow/pipelines/issues/1933), [3412](https://github.com/kubeflow/pipelines/issues/3412); and [this](https://stackoverflow.com/a/63219053) stackoverflow uses kubeflow v1, but vertex requires kubeflow v2 and where this doesn't work)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hyperkubeflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b446f81a2e8e0f661bfe3e51a4b4909fea4b44f7718487d79e51d97027e0877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
