{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example that shows how `pseudo_tuple_component.py` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "with open(\"vertex_config.json\", \"r\") as f:\n",
    "    gcp_cfg = json.load(f)  # I put GCP related stuff in here\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    gcp_cfg[\"credentials_path\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does not work:\n",
    "\n",
    "```python\n",
    "@dsl.component\n",
    "def my_transformer_op(item: str) -> str:\n",
    "    return item + \"_transformed\"\n",
    " \n",
    "@dsl.component\n",
    "def my_aggregator_op(args: list) -> str:\n",
    "    return \" \".join(args)\n",
    "\n",
    "@dsl.pipeline(\"aggtest\", \"agg test\")\n",
    "def dynamic_pipeline():\n",
    "    transformed_vals = []\n",
    "    for x in [\"a\", \"b\", \"c\"]:\n",
    "        transformed_vals.append(my_transformer_op(x))\n",
    "    my_aggregator_op([x.output for x in transformed_vals])\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=dynamic_pipeline, package_path=\"my_pipeline.yaml\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "TypeError: Object of type PipelineParam is not JSON serializable\n",
    "```\n",
    "\n",
    "### The `pseudo_tuple_component` workaround does work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "\n",
    "from pseudo_tuple_component import PseudoTuple, pseudo_tuple_component\n",
    "\n",
    "\n",
    "MY_LIST = [\"a\", \"b\", \"c\"]\n",
    "PIPELINE_NAME = \"pseudo-tuple-example\"\n",
    "\n",
    "\n",
    "@dsl.component\n",
    "def my_transformer_op(item: str) -> str:\n",
    "    return item + \"_transformed\"\n",
    "\n",
    "\n",
    "@pseudo_tuple_component(globals_=globals(), locals_=locals())\n",
    "def my_aggregator_op(args: PseudoTuple(len(MY_LIST), str)) -> str:\n",
    "    return \" \".join(args)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\"aggtest\", \"agg test\")\n",
    "def dynamic_pipeline():\n",
    "    transformed_vals = []\n",
    "    for x in MY_LIST:\n",
    "        transformed_vals.append(my_transformer_op(x).output)\n",
    "    my_aggregator_op(*transformed_vals)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=dynamic_pipeline, package_path=f\"{PIPELINE_NAME}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def my_aggregator_op(args_0: str, args_1: str, args_2: str) -> str:\n",
      "    args = (args_0, args_1, args_2)\n",
      "    return ' '.join(args)\n"
     ]
    }
   ],
   "source": [
    "from pseudo_tuple_component import expand_PseudoTuple_annotated_args_to_str\n",
    "\n",
    "\n",
    "def my_aggregator_op(args: PseudoTuple(len(MY_LIST), str)) -> str:\n",
    "    return \" \".join(args)\n",
    "\n",
    "\n",
    "new_source_code_for_func = expand_PseudoTuple_annotated_args_to_str(\n",
    "    my_aggregator_op, globals_=globals(), locals_=locals()\n",
    ")\n",
    "print(new_source_code_for_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}_job\",\n",
    "    credentials=credentials,\n",
    "    template_path=f\"{PIPELINE_NAME}.json\",\n",
    "    job_id=f\"{PIPELINE_NAME}-{TIMESTAMP}\",\n",
    "    pipeline_root=gcp_cfg[\"pipeline_root\"],\n",
    "    enable_caching=True,\n",
    "    project=gcp_cfg[\"project_id\"],\n",
    "    location=gcp_cfg[\"region\"],\n",
    ")\n",
    "job.submit(\n",
    "    service_account=gcp_cfg[\"service_account\"], experiment=gcp_cfg[\"experiment_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./pseudo_tuple_example.png\" width=800>\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hyperkubeflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b446f81a2e8e0f661bfe3e51a4b4909fea4b44f7718487d79e51d97027e0877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
